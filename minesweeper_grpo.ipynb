{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper LLM Competition - Custom GRPO Training\n",
    "\n",
    "## Goal\n",
    "Finetune an LLM with LoRA using GRPO to play Minesweeper by:\n",
    "- **Input**: JSON game state (board configuration)\n",
    "- **Output**: JSON action (reveal or flag a cell)\n",
    "\n",
    "Teams will compete to train the best Minesweeper-playing LLM!\n",
    "\n",
    "## Training Approach\n",
    "- **Model**: GPT-OSS 20B with LoRA or other models in the /root/.cache/huggingface/hub directory [**Any model other than /root/.cache/huggingface/hub will lead to disqualification**]\n",
    "- **Method**: GRPO (Group Relative Policy Optimization), SFT or any RL-policies (not just strict to use GRPO)\n",
    "- **Framework**: Unsloth (2-6x faster, 70% less VRAM)\n",
    "- **Hardware**: AMD GPU (ROCm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model with Unsloth\n",
    "\n",
    "Load GPT-OSS 20B with LoRA configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 1024  # Max context length\n",
    "lora_rank = 16         # LoRA rank (higher = smarter but slower; 4 is too low for reasoning tasks)\n",
    "\n",
    "# Try loading with explicit torch_dtype\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b-BF16\",\n",
    "    load_in_4bit = False,\n",
    "    max_seq_length = max_seq_length,\n",
    "    torch_dtype = torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Force model to cuda explicitly\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add LoRA Adapters\n",
    "\n",
    "Add LoRA layers for efficient finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper Game Implementation\n",
    "\n",
    "Custom Minesweeper environment supporting:\n",
    "- Customizable board size and mine count\n",
    "- Actions: reveal or flag cells\n",
    "- Win: reveal all safe cells\n",
    "- Lose: reveal a mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Optional, Set\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class MinesweeperGame:\n",
    "    rows: int\n",
    "    cols: int\n",
    "    num_mines: int\n",
    "    seed: Optional[int] = None\n",
    "    _rng: random.Random = field(init=False, repr=False)\n",
    "    _board: List[List[int]] = field(init=False, repr=False)  # -1 = mine, 0-8 = count\n",
    "    _revealed: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n",
    "    _flagged: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n",
    "    _state: str = field(default=\"ongoing\", init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.num_mines >= self.rows * self.cols:\n",
    "            raise ValueError(\"Too many mines for board size\")\n",
    "        self._rng = random.Random(self.seed)\n",
    "        self._board = [[0 for _ in range(self.cols)] for _ in range(self.rows)]\n",
    "        self._place_mines()\n",
    "        self._calculate_numbers()\n",
    "\n",
    "    def _place_mines(self):\n",
    "        \"\"\"Place mines randomly on the board\"\"\"\n",
    "        positions = [(r, c) for r in range(self.rows) for c in range(self.cols)]\n",
    "        mine_positions = self._rng.sample(positions, self.num_mines)\n",
    "        for r, c in mine_positions:\n",
    "            self._board[r][c] = -1\n",
    "\n",
    "    def _calculate_numbers(self):\n",
    "        \"\"\"Calculate numbers for each cell based on adjacent mines\"\"\"\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                if self._board[r][c] == -1:\n",
    "                    continue\n",
    "                count = 0\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = r + dr, c + dc\n",
    "                        if 0 <= nr < self.rows and 0 <= nc < self.cols:\n",
    "                            if self._board[nr][nc] == -1:\n",
    "                                count += 1\n",
    "                self._board[r][c] = count\n",
    "\n",
    "    def _reveal_cell(self, row: int, col: int) -> bool:\n",
    "        \"\"\"Reveal a cell. Returns True if valid move, False if invalid.\n",
    "        Uses iterative flood-fill to avoid recursion limit on large boards.\n",
    "        (Issue #11: was recursive; Issue typo: fixed 'bself' -> 'self')\n",
    "        \"\"\"\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            return False\n",
    "        if (row, col) in self._revealed or (row, col) in self._flagged:\n",
    "            return False\n",
    "\n",
    "        stack = [(row, col)]\n",
    "        while stack:\n",
    "            r, c = stack.pop()\n",
    "            if (r, c) in self._revealed:\n",
    "                continue\n",
    "\n",
    "            self._revealed.add((r, c))\n",
    "\n",
    "            # Hit a mine!\n",
    "            if self._board[r][c] == -1:\n",
    "                self._state = \"failed\"\n",
    "                return True\n",
    "\n",
    "            # Auto-reveal neighbors if cell is 0\n",
    "            if self._board[r][c] == 0:\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = r + dr, c + dc\n",
    "                        if (0 <= nr < self.rows and 0 <= nc < self.cols\n",
    "                                and (nr, nc) not in self._revealed\n",
    "                                and (nr, nc) not in self._flagged):\n",
    "                            stack.append((nr, nc))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _flag_cell(self, row: int, col: int) -> bool:\n",
    "        \"\"\"Flag/unflag a cell. Returns True if valid, False if invalid\"\"\"\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            return False\n",
    "        if (row, col) in self._revealed:\n",
    "            return False\n",
    "        \n",
    "        if (row, col) in self._flagged:\n",
    "            self._flagged.remove((row, col))\n",
    "        else:\n",
    "            self._flagged.add((row, col))\n",
    "        return True\n",
    "\n",
    "    def do_action(self, action: dict) -> str:\n",
    "        \"\"\"Execute an action and return a status string.\n",
    "\n",
    "        Returns one of:\n",
    "          'ok'               - valid move executed\n",
    "          'mine'             - revealed a mine (game over)\n",
    "          'win'              - game won after this move\n",
    "          'invalid_format'   - bad action dict / missing keys / bad types\n",
    "          'out_of_bounds'    - coordinates outside the board\n",
    "          'already_revealed' - cell was already revealed\n",
    "          'flagged_cell'     - tried to reveal a flagged cell\n",
    "          'invalid_flag'     - tried to flag a revealed cell\n",
    "          'game_over'        - game was already over before this call\n",
    "\n",
    "        (Issue #13: previously set state='failed' for ALL invalid moves,\n",
    "         conflating formatting errors with hitting a mine.)\n",
    "        \"\"\"\n",
    "        if self._state != \"ongoing\":\n",
    "            return \"game_over\"\n",
    "\n",
    "        if not isinstance(action, dict):\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        action_type = action.get(\"type\")\n",
    "        row = action.get(\"row\")\n",
    "        col = action.get(\"col\")\n",
    "\n",
    "        if action_type not in [\"reveal\", \"flag\"] or row is None or col is None:\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        try:\n",
    "            row, col = int(row), int(col)\n",
    "        except (ValueError, TypeError):\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            self._state = \"failed\"\n",
    "            return \"out_of_bounds\"\n",
    "\n",
    "        if action_type == \"reveal\":\n",
    "            if (row, col) in self._revealed:\n",
    "                self._state = \"failed\"\n",
    "                return \"already_revealed\"\n",
    "            if (row, col) in self._flagged:\n",
    "                self._state = \"failed\"\n",
    "                return \"flagged_cell\"\n",
    "            valid = self._reveal_cell(row, col)\n",
    "        else:\n",
    "            if (row, col) in self._revealed:\n",
    "                self._state = \"failed\"\n",
    "                return \"invalid_flag\"\n",
    "            valid = self._flag_cell(row, col)\n",
    "\n",
    "        if not valid:\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        self._check_win()\n",
    "\n",
    "        if self._state == \"failed\":\n",
    "            return \"mine\"\n",
    "        if self._state == \"success\":\n",
    "            return \"win\"\n",
    "        return \"ok\"\n",
    "\n",
    "    def _check_win(self):\n",
    "        \"\"\"Check if player has won\"\"\"\n",
    "        total_cells = self.rows * self.cols\n",
    "        safe_cells = total_cells - self.num_mines\n",
    "        if len(self._revealed) == safe_cells:\n",
    "            self._state = \"success\"\n",
    "\n",
    "    def get_visible_board(self) -> List[List[str]]:\n",
    "        \"\"\"Get board state as player sees it\"\"\"\n",
    "        visible = []\n",
    "        for r in range(self.rows):\n",
    "            row = []\n",
    "            for c in range(self.cols):\n",
    "                if (r, c) in self._flagged:\n",
    "                    row.append('F')\n",
    "                elif (r, c) in self._revealed:\n",
    "                    val = self._board[r][c]\n",
    "                    row.append('*' if val == -1 else str(val))\n",
    "                else:\n",
    "                    row.append('.')\n",
    "            visible.append(row)\n",
    "        return visible\n",
    "\n",
    "    def state(self) -> str:\n",
    "        return self._state\n",
    "\n",
    "    def pretty_print(self) -> str:\n",
    "        \"\"\"Pretty print the board\"\"\"\n",
    "        visible = self.get_visible_board()\n",
    "        lines = []\n",
    "        \n",
    "        # Header\n",
    "        header = \"   \" + \" \".join(f\"{i:2d}\" for i in range(self.cols))\n",
    "        lines.append(header)\n",
    "        lines.append(\"  \" + \"─\" * (self.cols * 3 + 1))\n",
    "        \n",
    "        # Board\n",
    "        for r, row in enumerate(visible):\n",
    "            line = f\"{r:2d}│ \" + \"  \".join(row)\n",
    "            lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test game\n",
    "game = MinesweeperGame(rows=6, cols=6, num_mines=5)\n",
    "print(game.pretty_print())\n",
    "print(f\"State: {game.state()}\")\n",
    "\n",
    "# Test action\n",
    "game.do_action({\"type\": \"reveal\", \"row\": 0, \"col\": 0})\n",
    "print(\"\\nAfter revealing (0,0):\")\n",
    "print(game.pretty_print())\n",
    "print(f\"State: {game.state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Input/Output Format\n",
    "\n",
    "## Input Format (Game State)\n",
    "```json\n",
    "{\n",
    "  \"board\": [\n",
    "    [\"1\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"]\n",
    "  ],\n",
    "  \"rows\": 6,\n",
    "  \"cols\": 6,\n",
    "  \"mines\": 5,\n",
    "  \"flags_placed\": 0,\n",
    "  \"cells_revealed\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "## Output Format (Action)\n",
    "```json\n",
    "{\"type\": \"reveal\", \"row\": 2, \"col\": 3}\n",
    "```\n",
    "or\n",
    "```json\n",
    "{\"type\": \"flag\", \"row\": 1, \"col\": 4}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "Important Hints:\n",
    "\n",
    "1. Prompt is crucial - make sure your LLM is not verbose and do not write/output reasoning, instead the verbose must be hidden or abstracted and\n",
    "    output must be JSON object - the verbosity in our experiment led to running out of max tokens set and\n",
    "    thus JSON parsing failure - i.e. Disqualification:\n",
    "    {{\"type\": \"reveal\", \"row\": <row_index>, \"col\": <col_index>}}\n",
    "    or\n",
    "    {{\"type\": \"flag\", \"row\": <row_index>, \"col\": <col_index>}}\n",
    "\n",
    "2. Make sure your model learns generic N*M game board shapes and # number of mines\n",
    "\n",
    "3. Do not flag the cell which is already flagged - game will go in recursion and you will have heavy penalty\n",
    "\n",
    "4. Do not flag the cell which is already revealed - game will go in recursion and you will have heavy penalty\n",
    "'''\n",
    "\n",
    "def format_state_for_llm(game: MinesweeperGame) -> str:\n",
    "    \"\"\"Convert game state to JSON prompt for LLM\"\"\"\n",
    "    state = {\n",
    "        \"board\": game.get_visible_board(),\n",
    "        \"rows\": game.rows,\n",
    "        \"cols\": game.cols,\n",
    "        \"mines\": game.num_mines,\n",
    "        \"flags_placed\": len(game._flagged),\n",
    "        \"cells_revealed\": len(game._revealed),\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"You are playing Minesweeper. Analyze the game state and output your next move.\n",
    "\n",
    "Game state:\n",
    "{json.dumps(state, indent=2)}\n",
    "\n",
    "Legend:\n",
    "- \".\" = unrevealed cell\n",
    "- \"F\" = flagged cell (suspected mine)\n",
    "- \"0\"-\"8\" = number of adjacent mines\n",
    "- \"*\" = revealed mine (game over)\n",
    "\n",
    "Output your next action as JSON:\n",
    "{{\"type\": \"reveal\", \"row\": <row_index>, \"col\": <col_index>}}\n",
    "or\n",
    "{{\"type\": \"flag\", \"row\": <row_index>, \"col\": <col_index>}}\n",
    "\n",
    "Your action:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def parse_llm_action(response: str) -> dict:\n",
    "    \"\"\"Extract JSON action from LLM response.\n",
    "    \n",
    "    Finds all JSON-like objects and returns the LAST one matching the\n",
    "    expected schema.  LLMs typically reason through options and place\n",
    "    their final answer at the end, so taking the last valid match is\n",
    "    more robust than taking the first.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    best = None\n",
    "    for match in re.finditer(r'\\{[^{}]*\\}', response):\n",
    "        try:\n",
    "            action = json.loads(match.group())\n",
    "            if (\"type\" in action and \"row\" in action and \"col\" in action\n",
    "                    and action[\"type\"] in [\"reveal\", \"flag\"]):\n",
    "                best = action\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return best\n",
    "\n",
    "# Test formatting\n",
    "game = MinesweeperGame(rows=6, cols=6, num_mines=5)\n",
    "prompt = format_state_for_llm(game)\n",
    "print(prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model Before Training\n",
    "\n",
    "See how the base model performs without finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=42)\n",
    "prompt = format_state_for_llm(game)\n",
    "\n",
    "# Removed reasoning_effort=\"low\" — GRPOTrainer does NOT pass it\n",
    "# during training, so using it only at eval creates a train/eval mismatch.\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    ")\n",
    "\n",
    "print(\"=== Base Model Response ===\")\n",
    "output = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 1.0,\n",
    "    max_new_tokens = 128,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Reward Functions\n",
    "\n",
    "Define reward functions to guide the model's learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def valid_json_reward(completions, **kwargs):\n",
    "    \"\"\"Reward valid JSON action format\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            scores.append(-5.0)  # Invalid format\n",
    "        else:\n",
    "            scores.append(1.0)   # Valid format\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def gameplay_scores(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Scoring Criteria:\n",
    "    1.  Flag cell that IS a mine        → +15\n",
    "    2.  Flag cell that is NOT a mine    → -10\n",
    "    3.  Reveal cell that IS a mine      → -25 (round over, team goes to next round)\n",
    "    4.  Reveal cell that is safe        → +10 or +15 (+10 is for randomly guessed OR +15 if logically deducible)\n",
    "    5.  Flag already flagged cell       → -12\n",
    "    6.  Reveal already revealed cell    → -12\n",
    "    7.  Out of bounds                   → -15\n",
    "    8.  Total flags > total mines       → -10\n",
    "    9.  Invalid JSON                    → -50\n",
    "    10. Win the game                    → +100 (big bonus) - Winning here means Flagging all the mines + Revealing all the safe cells\n",
    "    11. Reveal a flagged cell           → -8\n",
    "    12. Flag a revealed cell            → -8\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    # Get game state info passed from dataset\n",
    "    seeds = kwargs.get(\"seed\", [])\n",
    "    move_histories = kwargs.get(\"move_history\", [])\n",
    "\n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"]\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        # Criterion 9: Invalid JSON\n",
    "        if action is None:\n",
    "            scores.append(-10.0)\n",
    "            continue\n",
    "\n",
    "        # Reconstruct EXACT game state\n",
    "        if idx < len(seeds) and idx < len(move_histories):\n",
    "            seed = seeds[idx]\n",
    "            move_history_raw = move_histories[idx]\n",
    "\n",
    "            # move_history is stored as a JSON string in the dataset\n",
    "            # to avoid HF Dataset schema inference mangling list-of-dicts.\n",
    "            if isinstance(move_history_raw, str):\n",
    "                move_history = json.loads(move_history_raw)\n",
    "            else:\n",
    "                move_history = move_history_raw  # backward compat\n",
    "\n",
    "            # Reconstruct game to exact state from prompt - not for learning purposes\n",
    "            game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=seed)\n",
    "            for prev_action in move_history:\n",
    "                game.do_action(prev_action)\n",
    "\n",
    "            board = game.get_visible_board()\n",
    "            row, col = action[\"row\"], action[\"col\"]\n",
    "            action_type = action[\"type\"]\n",
    "\n",
    "            # Criterion 7: Out of bounds\n",
    "            if not (0 <= row < game.rows and 0 <= col < game.cols):\n",
    "                # TO BE ADDED scores.append() or your equation on reward/penalty\n",
    "                continue\n",
    "\n",
    "            # Criterion 1....N\n",
    "            '''\n",
    "            if <reward functions>\n",
    "            '''\n",
    "\n",
    "            # Unknown action type\n",
    "            else:\n",
    "                scores.append(-10.0)\n",
    "        else:\n",
    "            # Fallback\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Dataset\n",
    "\n",
    "Generate diverse game states for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def generate_game_states(num_samples=1000, rows=6, cols=6, num_mines=5,\n",
    "                         rng_seed=42):\n",
    "    \"\"\"\n",
    "    Generate EXACTLY num_samples diverse Minesweeper game states.\n",
    "    \n",
    "    Mix of:\n",
    "    - Fresh games (20-30%)\n",
    "    - Mid-game states (70-80%)\n",
    "    \n",
    "    IMPORTANTLY: Stores seed + move_history (as JSON string) so reward\n",
    "    function can reconstruct the EXACT game state!\n",
    "    \n",
    "    Keeps generating until we have exactly num_samples valid ongoing games.\n",
    "    \n",
    "    Args:\n",
    "        rng_seed: Seed for numpy/random RNG for reproducibility.\n",
    "    \"\"\"\n",
    "    # Seed RNG for reproducibility across runs\n",
    "    np.random.seed(rng_seed)\n",
    "    random.seed(rng_seed)\n",
    "\n",
    "    dataset_items = []\n",
    "    attempts = 0\n",
    "    max_attempts = num_samples * 3  # Safety limit\n",
    "    \n",
    "    while len(dataset_items) < num_samples and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        seed = np.random.randint(100000)\n",
    "        game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n",
    "        \n",
    "        # Make 0-5 random moves (0 = fresh game, 1-5 = mid-game)\n",
    "        num_moves = np.random.randint(0, 6)\n",
    "        move_history = []\n",
    "        \n",
    "        for _ in range(num_moves):\n",
    "            board = game.get_visible_board()\n",
    "            unrevealed = []\n",
    "            for r in range(rows):\n",
    "                for c in range(cols):\n",
    "                    if board[r][c] == '.':\n",
    "                        unrevealed.append((r, c))\n",
    "            \n",
    "            if unrevealed and game.state() == \"ongoing\":\n",
    "                r, c = random.choice(unrevealed)\n",
    "                action = {\"type\": \"reveal\", \"row\": r, \"col\": c}\n",
    "                game.do_action(action)\n",
    "                move_history.append(action)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Only add ongoing games (skip failed/completed games)\n",
    "        if game.state() == \"ongoing\":\n",
    "            prompt_text = format_state_for_llm(game)\n",
    "            dataset_items.append({\n",
    "                \"prompt\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                \"seed\": seed,  # Store seed to reconstruct game\n",
    "                # IMPORTANT: Serialize as JSON string to avoid HF Dataset\n",
    "                # schema inference mangling list-of-dicts into dict-of-lists\n",
    "                \"move_history\": json.dumps(move_history),\n",
    "            })\n",
    "    \n",
    "    return Dataset.from_list(dataset_items)\n",
    "\n",
    "# Generate training dataset\n",
    "print(\"Generating training dataset...\")\n",
    "dataset = generate_game_states(num_samples=1000, rows=6, cols=6, num_mines=5)\n",
    "print(f\"Created EXACTLY {len(dataset)} training examples (all ongoing games)\")\n",
    "\n",
    "# Count fresh vs mid-game\n",
    "fresh_count = sum(1 for item in dataset if item[\"move_history\"] == \"[]\")\n",
    "print(f\"  Fresh games: {fresh_count} ({fresh_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Mid-game states: {len(dataset) - fresh_count} ({(len(dataset)-fresh_count)/len(dataset)*100:.1f}%)\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample training prompt:\")\n",
    "print(dataset[0][\"prompt\"][0][\"content\"][:400] + \"...\")\n",
    "print(f\"Seed: {dataset[0]['seed']}, Previous moves: {len(json.loads(dataset[0]['move_history']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure GRPO Training\n",
    "\n",
    "Set up GRPO trainer with all hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Calculate max lengths\n",
    "max_prompt_length = 600   # JSON state prompt\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "# GRPO Configuration\n",
    "training_args = GRPOConfig(\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,  # Issue #6: was 1; 4 gives 16 effective completions per update\n",
    "    num_generations = 4,  # Generate 4 actions per state\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    max_steps = 500,      # Adjust based on compute budget\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"minesweeper_custom_outputs\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Generations per state: {training_args.num_generations}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  LoRA rank: {lora_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class MinesweeperEvalCallback(TrainerCallback):\n",
    "    \"\"\"Periodically play games during training and log win rate.\n",
    "    (Issue #8: no validation / reward tracking in the original notebook.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_every_steps=50, num_games=5):\n",
    "        self.eval_every_steps = eval_every_steps\n",
    "        self.num_games = num_games\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, processing_class=None, **kwargs):\n",
    "        if state.global_step % self.eval_every_steps != 0:\n",
    "            return\n",
    "\n",
    "        tokenizer = processing_class\n",
    "        if tokenizer is None or model is None:\n",
    "            return\n",
    "\n",
    "        # Temporarily set model to eval mode\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "\n",
    "        wins = 0\n",
    "        for i in range(self.num_games):\n",
    "            game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=10000 + i)\n",
    "            moves = 0\n",
    "            while game.state() == \"ongoing\" and moves < 50:\n",
    "                prompt = format_state_for_llm(game)\n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )\n",
    "                output = model.generate(\n",
    "                    **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "                    temperature=0.7,\n",
    "                    max_new_tokens=128,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "                response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                action = parse_llm_action(response)\n",
    "                if action is None:\n",
    "                    break\n",
    "                game.do_action(action)\n",
    "                moves += 1\n",
    "            if game.state() == \"success\":\n",
    "                wins += 1\n",
    "\n",
    "        win_rate = wins / self.num_games\n",
    "        print(f\"\\n[Eval @ step {state.global_step}] Win rate: {wins}/{self.num_games} ({win_rate*100:.0f}%)\\n\")\n",
    "\n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "eval_callback = MinesweeperEvalCallback(eval_every_steps=50, num_games=5)\n",
    "print(\"Eval callback created: plays 5 games every 50 steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "Start GRPO training with reward functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        valid_json_reward,   # Reward valid JSON format\n",
    "        gameplay_scores,     # Reward good gameplay\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "    callbacks = [eval_callback],  # Periodic gameplay evaluation\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Trained Model\n",
    "\n",
    "Evaluate the finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new game\n",
    "test_game = MinesweeperGame(rows=6, cols=6, num_mines=5)\n",
    "test_prompt = format_state_for_llm(test_game)\n",
    "\n",
    "# Removed reasoning_effort=\"low\" for train/eval consistency\n",
    "test_text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": test_prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    ")\n",
    "\n",
    "print(\"=== Trained Model Response ===\")\n",
    "output = model.generate(\n",
    "    **tokenizer(test_text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0.7,\n",
    "    max_new_tokens = 128,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")\n",
    "\n",
    "# Parse and test action\n",
    "response_text = tokenizer.decode(output[0])\n",
    "action = parse_llm_action(response_text)\n",
    "print(f\"\\nParsed action: {action}\")\n",
    "\n",
    "if action:\n",
    "    test_game.do_action(action)\n",
    "    print(f\"\\nGame state after action: {test_game.state()}\")\n",
    "    print(test_game.pretty_print())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Play Complete Games\n",
    "\n",
    "Test the model on multiple complete games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_full_game(model, tokenizer, rows=6, cols=6, num_mines=5, seed=None, max_moves=50):\n",
    "    \"\"\"Play a complete Minesweeper game with the model\"\"\"\n",
    "    game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n",
    "    moves = 0\n",
    "    \n",
    "    while game.state() == \"ongoing\" and moves < max_moves:\n",
    "        # Get current state\n",
    "        prompt = format_state_for_llm(game)\n",
    "        # Removed reasoning_effort=\"low\" for train/eval consistency\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize = False,\n",
    "            add_generation_prompt = True,\n",
    "        )\n",
    "        \n",
    "        # Generate action\n",
    "        output = model.generate(\n",
    "            **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "            temperature = 0.7,\n",
    "            max_new_tokens = 128,\n",
    "            do_sample = True,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(output[0])\n",
    "        action = parse_llm_action(response)\n",
    "        \n",
    "        if action is None:\n",
    "            break  # Invalid action\n",
    "        \n",
    "        game.do_action(action)\n",
    "        moves += 1\n",
    "    \n",
    "    return game, moves\n",
    "\n",
    "# Evaluate on 100 games for statistically meaningful win rates\n",
    "NUM_EVAL_GAMES = 100\n",
    "print(f\"Evaluating model on {NUM_EVAL_GAMES} games...\\n\")\n",
    "wins = 0\n",
    "total_moves = 0\n",
    "\n",
    "for i in range(NUM_EVAL_GAMES):\n",
    "    game, moves = play_full_game(model, tokenizer, seed=i)\n",
    "    result = game.state()\n",
    "\n",
    "    if result == \"success\":\n",
    "        wins += 1\n",
    "    # Only print individual results for first 10 + any wins\n",
    "    if i < 10 or result == \"success\":\n",
    "        tag = \"WIN\" if result == \"success\" else \"LOSS\"\n",
    "        print(f\"Game {i+1}: {tag} ({result}) after {moves} moves\")\n",
    "\n",
    "    total_moves += moves\n",
    "\n",
    "if NUM_EVAL_GAMES > 10:\n",
    "    print(f\"... (showing first 10 + wins; {NUM_EVAL_GAMES} total)\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Win rate: {wins}/{NUM_EVAL_GAMES} ({wins/NUM_EVAL_GAMES*100:.1f}%)\")\n",
    "print(f\"  Average moves: {total_moves/NUM_EVAL_GAMES:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model\n",
    "\n",
    "Save your trained model for competition submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"my_minesweeper_model\")\n",
    "tokenizer.save_pretrained(\"my_minesweeper_model\")\n",
    "\n",
    "print(\"Model saved to: my_minesweeper_model/\")\n",
    "\n",
    "# Save merged model in 16bit (local file name which will be used for eval)\n",
    "if False:\n",
    "    model.save_pretrained_merged(\n",
    "        \"my_minesweeper_model_merged\",\n",
    "        tokenizer,\n",
    "        save_method = \"merged_16bit\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Tips\n",
    "\n",
    "## Improve Your Model:\n",
    "\n",
    "1. **Adjust Reward Functions**\n",
    "   - Increase rewards for logical deduction\n",
    "   - Add penalties for random moves\n",
    "   - Reward flagging correct mines\n",
    "\n",
    "2. **Tune Hyperparameters**\n",
    "   - Increase `max_steps` for longer training\n",
    "   - Adjust `learning_rate` (try 1e-5 to 1e-4)\n",
    "   - Increase `lora_rank` for more capacity\n",
    "   - Adjust `num_generations` (2-8)\n",
    "\n",
    "3. **Better Training Data**\n",
    "   - Generate more diverse states\n",
    "   - Include harder scenarios (more mines)\n",
    "   - Add states requiring logical deduction\n",
    "\n",
    "4. **Advanced Techniques**\n",
    "   - Multi-step rollouts in reward function\n",
    "   - Curriculum learning (easy → hard boards)\n",
    "   - Ensemble multiple models\n",
    "\n",
    "## Team Strategy:\n",
    "- Experiment with different reward functions\n",
    "- Try different board sizes during training\n",
    "- Analyze failed games to improve rewards\n",
    "- Use temperature sampling during evaluation\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
