{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911c799d-79f3-4e67-b043-c13ebcfc4047",
   "metadata": {},
   "source": [
    "# Minesweeper RL Agent â€” GRPO Training & Evaluation\n",
    "# ================================================\n",
    "# This notebook demonstrates the complete pipeline:\n",
    "# 1. Training a Minesweeper agent using GRPO on AMD MI300X\n",
    "# 2. Evaluating the trained agent\n",
    "# 3. Demo gameplay\n",
    "#\n",
    "# To run: Copy cells into Jupyter Lab on the hackathon server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055c4e7-9576-4eef-9171-7aace08c31b1",
   "metadata": {},
   "source": [
    "# # ðŸŽ® Minesweeper RL Agent â€” GRPO on AMD MI300X\n",
    "# \n",
    "# **Team**: Team 69  \n",
    "# **Model**: `unsloth/gpt-oss-20b-BF16` (20B params)  \n",
    "# **Method**: GRPO (Group Relative Policy Optimization) with LoRA (r=16)  \n",
    "# **GPU**: AMD MI300X (274.5 GB VRAM)  \n",
    "# **Training**: 500 steps, ~4.6 hours  \n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8a0fb-4b22-4eb8-b23d-b566113e02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch, json, random, re\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d817d-0e9d-4263-8c0c-15083820e30c",
   "metadata": {},
   "source": [
    "# ## 2. Training Results Summary\n",
    "# \n",
    "# Our GRPO training showed clear learning progress:\n",
    "# \n",
    "# | Metric | Start (Step 10) | End (Step 500) | Change |\n",
    "# |--------|----------------|----------------|--------|\n",
    "# | Total Reward | -14.4 | +5.6 to +7.8 | +22 pts |\n",
    "# | Valid JSON % | ~60% | 100% | âœ… |\n",
    "# | Gameplay Score | -9.6 | +4.6 to +6.9 | +16 pts |\n",
    "# | Completion Clipping | 99% | 71% | Focused |\n",
    "# \n",
    "# ### Key Achievements:\n",
    "# - âœ… **100% valid JSON output** â€” model learned exact output format\n",
    "# - âœ… **Positive gameplay rewards** â€” model avoids mines, reveals safe cells\n",
    "# - âœ… **Logic deduction** â€” model uses numbered clues to deduce safe cells\n",
    "# - âœ… **LoRA efficiency** â€” only 7.96M/20.9B params trained (0.04%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e635c5-c577-40b7-b328-f09e40f59632",
   "metadata": {},
   "source": [
    "# ## 3. Reward System (12-Point Scoring)\n",
    "# \n",
    "# | Action | Reward |\n",
    "# |--------|--------|\n",
    "# | Flag a mine | +15 |\n",
    "# | Reveal safe cell | +10 |\n",
    "# | Reveal safe (logically deduced) | +15 |\n",
    "# | Win game | +100 |\n",
    "# | Flag safe cell | -10 |\n",
    "# | Reveal mine (death) | -25 |\n",
    "# | Out of bounds | -15 |\n",
    "# | Already revealed | -12 |\n",
    "# | Invalid JSON | -10 |\n",
    "# | Flag already flagged | -8 |\n",
    "# | Excess flags | -10 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7371f8-d12b-455c-b3b2-e8c4d988b0e6",
   "metadata": {},
   "source": [
    "# ## 4. Load Trained Model & Run Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f83cd-824b-46de-959c-ead4b4a6b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent dir to path for imports\n",
    "sys.path.insert(0, '/workspace')\n",
    "sys.path.insert(0, '/workspace/agents')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# LoRA implementation (matches training)\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear, r=16, alpha=32):\n",
    "        super().__init__()\n",
    "        self.original_linear = original_linear\n",
    "        self.scaling = alpha / r\n",
    "        in_f, out_f = original_linear.in_features, original_linear.out_features\n",
    "        original_linear.weight.requires_grad = False\n",
    "        if original_linear.bias is not None: original_linear.bias.requires_grad = False\n",
    "        self.lora_A = nn.Linear(in_f, r, bias=False, dtype=torch.bfloat16)\n",
    "        self.lora_B = nn.Linear(r, out_f, bias=False, dtype=torch.bfloat16)\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=np.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "    def forward(self, x):\n",
    "        return self.original_linear(x) + self.lora_B(self.lora_A(x.to(self.lora_A.weight.dtype))) * self.scaling\n",
    "\n",
    "def apply_lora(model, r=16, alpha=32):\n",
    "    targets = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "    for name, mod in list(model.named_modules()):\n",
    "        for t in targets:\n",
    "            if name.endswith(t) and isinstance(mod, nn.Linear):\n",
    "                parts = name.split('.')\n",
    "                parent = model\n",
    "                for p in parts[:-1]: parent = getattr(parent, p)\n",
    "                dev = mod.weight.device\n",
    "                setattr(parent, parts[-1], LoRALinear(mod, r, alpha).to(dev))\n",
    "                break\n",
    "    return model\n",
    "\n",
    "# %%\n",
    "print(\">>> Loading model...\")\n",
    "MODEL = \"unsloth/gpt-oss-20b-BF16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)\n",
    "model = apply_lora(model, r=16, alpha=32)\n",
    "\n",
    "# Load trained weights\n",
    "lora_path = \"/workspace/your_finetuned_model/lora_weights.pt\"\n",
    "state = torch.load(lora_path, map_location=\"cpu\", weights_only=True)\n",
    "ms = model.state_dict()\n",
    "loaded = sum(1 for k,v in state.items() if k in ms and ms[k].copy_(v.to(ms[k].device)) is not None)\n",
    "print(f\"  Loaded {loaded} LoRA tensors\")\n",
    "model.eval()\n",
    "print(\">>> Model ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964cc306-a6c4-49f6-b513-ce8f01cb2c3b",
   "metadata": {},
   "source": [
    "# ## 5. Demo: Watch the Agent Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8ce0f-2c70-4ef7-9c09-eb429af76b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Optional, Set\n",
    "\n",
    "@dataclass\n",
    "class MinesweeperGame:\n",
    "    rows: int; cols: int; num_mines: int; seed: Optional[int] = None\n",
    "    _rng: random.Random = field(init=False, repr=False)\n",
    "    _board: List[List[int]] = field(init=False, repr=False)\n",
    "    _revealed: Set[Tuple[int,int]] = field(init=False, repr=False, default_factory=set)\n",
    "    _flagged: Set[Tuple[int,int]] = field(init=False, repr=False, default_factory=set)\n",
    "    _state: str = field(default=\"ongoing\", init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._rng = random.Random(self.seed)\n",
    "        self._board = [[0]*self.cols for _ in range(self.rows)]\n",
    "        pos = [(r,c) for r in range(self.rows) for c in range(self.cols)]\n",
    "        for r,c in self._rng.sample(pos, self.num_mines): self._board[r][c] = -1\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                if self._board[r][c]==-1: continue\n",
    "                self._board[r][c] = sum(1 for dr in [-1,0,1] for dc in [-1,0,1]\n",
    "                    if (dr or dc) and 0<=r+dr<self.rows and 0<=c+dc<self.cols and self._board[r+dr][c+dc]==-1)\n",
    "\n",
    "    def _reveal(self, r, c):\n",
    "        if (r,c) in self._revealed or (r,c) in self._flagged: return\n",
    "        stack = [(r,c)]\n",
    "        while stack:\n",
    "            r,c = stack.pop()\n",
    "            if (r,c) in self._revealed: continue\n",
    "            self._revealed.add((r,c))\n",
    "            if self._board[r][c]==-1: self._state=\"failed\"; return\n",
    "            if self._board[r][c]==0:\n",
    "                for dr in [-1,0,1]:\n",
    "                    for dc in [-1,0,1]:\n",
    "                        if (dr or dc) and 0<=r+dr<self.rows and 0<=c+dc<self.cols and (r+dr,c+dc) not in self._revealed:\n",
    "                            stack.append((r+dr,c+dc))\n",
    "\n",
    "    def do_action(self, a):\n",
    "        if self._state!=\"ongoing\": return \"game_over\"\n",
    "        try: t,r,c = a[\"type\"],int(a[\"row\"]),int(a[\"col\"])\n",
    "        except: self._state=\"failed\"; return \"invalid\"\n",
    "        if not(0<=r<self.rows and 0<=c<self.cols): self._state=\"failed\"; return \"oob\"\n",
    "        if t==\"reveal\":\n",
    "            if (r,c) in self._revealed: self._state=\"failed\"; return \"already\"\n",
    "            self._reveal(r,c)\n",
    "        elif t==\"flag\":\n",
    "            if (r,c) in self._revealed: self._state=\"failed\"; return \"bad_flag\"\n",
    "            self._flagged.symmetric_difference_update({(r,c)})\n",
    "        if self._state==\"failed\": return \"mine\"\n",
    "        if len(self._revealed)==self.rows*self.cols-self.num_mines: self._state=\"success\"; return \"win\"\n",
    "        return \"ok\"\n",
    "\n",
    "    def visible(self):\n",
    "        v = []\n",
    "        for r in range(self.rows):\n",
    "            row = []\n",
    "            for c in range(self.cols):\n",
    "                if (r,c) in self._flagged: row.append('F')\n",
    "                elif (r,c) in self._revealed: row.append('*' if self._board[r][c]==-1 else str(self._board[r][c]))\n",
    "                else: row.append('.')\n",
    "            v.append(row)\n",
    "        return v\n",
    "\n",
    "    def display(self):\n",
    "        print(\"   \"+\" \".join(str(c) for c in range(self.cols)))\n",
    "        for r,row in enumerate(self.visible()):\n",
    "            print(f\" {r}| \"+\" \".join(row)+\" |\")\n",
    "        print(f\"   State: {self._state} | Revealed: {len(self._revealed)} | Flags: {len(self._flagged)}\")\n",
    "\n",
    "def make_prompt(game):\n",
    "    s = {\"board\":game.visible(),\"rows\":game.rows,\"cols\":game.cols,\"mines\":game.num_mines,\n",
    "         \"flags_placed\":len(game._flagged),\"cells_revealed\":len(game._revealed)}\n",
    "    return f'You are playing Minesweeper. Analyze the game state and output your next move.\\n\\nGame state:\\n{json.dumps(s,indent=2)}\\n\\nOutput your next action as JSON:\\n{{\"type\": \"reveal\", \"row\": <row>, \"col\": <col>}}\\nor\\n{{\"type\": \"flag\", \"row\": <row>, \"col\": <col>}}\\n\\nYour action:'\n",
    "\n",
    "@torch.no_grad()\n",
    "def agent_move(game, model, tokenizer):\n",
    "    prompt = make_prompt(game)\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out = model.generate(**inp, max_new_tokens=128, temperature=0.3, top_p=0.9, do_sample=True,\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "    resp = tokenizer.decode(out[0][inp[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    for m in re.finditer(r'\\{[^{}]*\\}', resp):\n",
    "        try:\n",
    "            a = json.loads(m.group())\n",
    "            if \"type\" in a and \"row\" in a and \"col\" in a: return a, resp\n",
    "        except: pass\n",
    "    return None, resp\n",
    "\n",
    "# %%\n",
    "# Play 3 demo games\n",
    "for seed in [42, 123, 456]:\n",
    "    game = MinesweeperGame(6, 6, 5, seed)\n",
    "    print(f\"\\n{'='*40}\\n  GAME (seed={seed})\\n{'='*40}\")\n",
    "    game.display()\n",
    "    moves = 0\n",
    "    while game._state == \"ongoing\" and moves < 20:\n",
    "        action, raw = agent_move(game, model, tokenizer)\n",
    "        if action is None:\n",
    "            print(f\"  Move {moves+1}: INVALID OUTPUT\"); break\n",
    "        result = game.do_action(action)\n",
    "        moves += 1\n",
    "        print(f\"\\n  Move {moves}: {json.dumps(action)} â†’ {result}\")\n",
    "        game.display()\n",
    "    print(f\"\\n  RESULT: {game._state} in {moves} moves\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c843a-5ff8-4f71-a8ce-1304ba459aa2",
   "metadata": {},
   "source": [
    "# ## 6. Batch Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e4571-e0c5-4448-957a-7c0f6cc4eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(20):\n",
    "    game = MinesweeperGame(6, 6, 5, seed=10000+i)\n",
    "    moves = 0; reward = 0\n",
    "    while game._state==\"ongoing\" and moves < 30:\n",
    "        action, _ = agent_move(game, model, tokenizer)\n",
    "        if not action: reward -= 10; break\n",
    "        r = game.do_action(action)\n",
    "        moves += 1\n",
    "        if r==\"ok\": reward += 10\n",
    "        elif r==\"win\": reward += 100\n",
    "        elif r==\"mine\": reward -= 25\n",
    "        else: reward -= 10\n",
    "    results.append({\"state\": game._state, \"moves\": moves, \"reward\": reward})\n",
    "\n",
    "wins = sum(1 for r in results if r[\"state\"]==\"success\")\n",
    "avg_r = np.mean([r[\"reward\"] for r in results])\n",
    "avg_m = np.mean([r[\"moves\"] for r in results])\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"  EVALUATION: {len(results)} games\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Win Rate: {wins}/{len(results)} ({100*wins/len(results):.1f}%)\")\n",
    "print(f\"  Avg Reward: {avg_r:+.1f}\")\n",
    "print(f\"  Avg Moves: {avg_m:.1f}\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56030604-1acf-4b13-91e0-f73e430d1f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Architecture Summary\n",
    "# \n",
    "# ```\n",
    "# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "# â”‚   unsloth/gpt-oss-20b-BF16     â”‚\n",
    "# â”‚   (20B params, BFloat16)       â”‚\n",
    "# â”‚                                 â”‚\n",
    "# â”‚   + LoRA Adapters (r=16)       â”‚\n",
    "# â”‚     7.96M trainable params     â”‚\n",
    "# â”‚     96 target layers            â”‚\n",
    "# â”‚                                 â”‚\n",
    "# â”‚   Trained with GRPOTrainer     â”‚\n",
    "# â”‚   500 steps, batch=4, GA=4     â”‚\n",
    "# â”‚   LR: 5e-5 with cosine decay  â”‚\n",
    "# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "#          â†• generates moves\n",
    "# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "# â”‚   Minesweeper Engine (6x6, 5M) â”‚\n",
    "# â”‚   12-point reward system       â”‚\n",
    "# â”‚   Logic deduction bonus        â”‚\n",
    "# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "# `''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
